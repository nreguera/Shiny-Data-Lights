
```{r SET, eval=TRUE, include=FALSE}
# General setup parameters

# SET: Settings
# LDA: Load Data
# CLN: Clean Data
# PRP: Process Data
# EDA: Explore Data
# MOD: Model Data

# Set library paths
myPaths <- .libPaths() 
myPaths <- c("E:/R/Libraries/4.0.2", myPaths[1])
.libPaths(myPaths)
.libPaths()

# General chunk parameters
knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE, 
                      eval = TRUE, 
                      include = FALSE, 
                      echo = FALSE,
                      cache = FALSE)

# Clean and load global environment
rm(list = ls())
file = paste0(dirname(getwd()), "/myEnvironment.RData", sep="")
load(file)

# remove the vatiables that we don´t want
rm(file)

```


```{r SET}
# Load libraries

# Load data
library(readxl)
library(Rnightlights)

# Process data
library(sp)
library(tidyverse)
library(lubridate)
library(reshape)
library(rgdal)
require(rgeos)
library(sf)
library(rmapshaper)

# Explore data
library(ggplot2)
library(tmap)
library(tmaptools)
library(leaflet)
library(plotly)
library(ggExtra)
library(tidyr) 

# Model data
library(DAAG)
library(mclust)

# Performance
library(shinyloadtest)

# Clear console
cat("\014")

```


```{r SET}

# Directories
dir_cache = "Cache/"
dir_outputs = "Outputs/"
dir_data = "Data/"

```


```{r SET, eval=FALSE}
# Define a function to extract the month

expand_dates <- function(start, end) {

    # the number of entries we want to add
  to_add <- month(end) - month(start) 

  # Take the start date, roll it forwards until the month is equal to the end month
  start_dates <- start + months(0:to_add)

  # everything but the first start_date is rolled back to first of month
  start_dates <- c(start_dates[1],
                   rollback(start_dates[-1], roll_to_first = T))

  # end dates are just the start_dates rolled forwards to the end of the month
  # apply to all but last, thats the end date
  end_dates <- c(rollback(ceiling_date(start_dates[-length(start_dates)], unit = "months")), end)

  data.frame(start_dates = start_dates,
             end_dates = end_dates)
}


```


```{r SET}
# A function factory for getting integer y-axis values.
integer_breaks <- function(n = 5, ...) {
  
  fxn <- function(x) {
    breaks <- floor(pretty(x, n, ...))
    names(breaks) <- attr(breaks, "labels")
    breaks
  }
  
  return(fxn)
}

```


```{r SET}

# Parameters
parameter_startDate = "2012-04-01"
parameter_endDate = "2019-12-01"
parameter_date = as.Date("2012-04-01")
parameter_country = "MMR" # (ISO3) country code
parameter_region = "Kachin"

```


```{r LDA}
# Load geographical data

# Region/State level
file <- paste(dir_data, "geo/gadm36_MMR_1_sp.rds", sep="")
rg_geo <- readRDS(file)

# Township level
file <- paste(dir_data, "geo/gadm36_MMR_3_sp.rds", sep="")
tw_geo <- readRDS(file)

# Township level (MIMU)
#file <- "E:/Data Lights/Data/geo/mmr_polbnda_250k_adm3_mimu/mmr_polbnda_250k_adm3_mimu.shp"
#tw_geo_MIMU <- st_read(file)

# remove the variables that we don´t need
rm(file)

```


```{r PRP}
# Create new dataframes from the geographical data

# create a list of regions and states
rg_list = as.data.frame(rg_geo@data$NAME_1)

# There are 2 townships with the same name in the same Region, so we will use for the Maubin region
# township the name that appears in the VARNAME_3 column: "Danuphyu".
tw_geo@data$NAME_3[tw_geo@data$NAME_2=="Maubin" & tw_geo@data$NAME_3=="Danubyu"] <- "Danuphyu"

# create a list of regions and townships
tw_list = tw_geo@data[,c(4,7,10)]
tw_list <- cbind(id=rownames(tw_list), tw_list)


```


```{r PRP}
# Simplify the polygons in the map layers

# Region/state level
#rg_geo = ms_simplify(rg_geo, keep = 0.1)

# Township level
#tw_geo = ms_simplify(tw_geo, keep = 0.1)

```


```{r SET, eval=FALSE}
# Setup specific libraries parameters

# Regional setup
Sys.setlocale("LC_TIME", "English")
Sys.setlocale(locale="UTF-8")

# Setup to improve performance downloading data for Rnightlights
pkgOptions(downloadMethod="aria", extractMethod="rast", numCores=4, deleteTiles=TRUE)

# to know which admLevel select in the getCtryNlData function AND create the CSV structure
searchAdmLevel(ctryCode = "MMR", 
               custPolyPath = paste0(dir_data,
                                     "/geo/MIMU/mmr_polbnda_250k_adm3_mimu.zip",
                                     sep="",
                                     collapse=NULL))

```


```{r LDA, eval=FALSE}
# Download Nightlights data by country

# Download rasters (tif) and radiance readings (csv)
nl_source <- getCtryNlData(
  ctryCode = parameter_country, # country
  custPolyPath = "mmr_polbnda_250k_adm3_mimu.zip",
  admLevel = "0_mmr_polbnda_250k_adm3_mimu", # admin level
  nlType = "VIIRS.M", # monthly values
  nlPeriods = nlRange("202006", "202012"),
  ignoreMissing = FALSE, # not ignoring missing values
  nlStats = list("mean", na.rm=TRUE) # aggregation: average 
)

``` 


```{r LDA}
# Load nightlights data

file <- "E:/Data Lights/Data/nl/NL_DATA_MMR_ADM3_GADM-3.6-SHPZIP.csv"

nl <- read.csv(file=file,
               header=TRUE, 
               sep=",",
               stringsAsFactors = FALSE)

# remove the variables that we don´t need
rm(file)

```


```{r CLN}
# Clean Nightlights dataset

nl_data <- as.data.frame(nl)

# Change the names of the nightlights dataset columns
names(nl_data) [names(nl_data) == "country"] <- "country"
names(nl_data) [names(nl_data) == "division_.yin."] <- "region"
names(nl_data) [names(nl_data) == "district_.kayaing."] <- "district"
names(nl_data) [names(nl_data) == "village.township"] <- "township"
names(nl_data) [names(nl_data) == "area_sq_km"] <- "area"

# No NAs

# There are 2 townships with the same name in the same region, so we will use for the Maubin region
# township the name that appears in the VARNAME_3 column: "Danuphyu"
nl_data[14, 4] <- "Danuphyu"

```


```{r PRP}
# Complete with NAs the months of 2012 where there are not observations

# Easy way: adding those columns to the current data at this point, using the same name template
#nl_name = names(nl_data)[6]
# NL_VIIRS.M_VCMCFG.MTSALL.MEAN.RGFT_201204_mean.....na.rm.TRUE.

# We need to add 3 months (3 new columns) from january to march 2012
nl_data$NL_VIIRS.M_VCMCFG.MTSALL.MEAN.RGFT_201201_mean.....na.rm.TRUE. = NA
nl_data$NL_VIIRS.M_VCMCFG.MTSALL.MEAN.RGFT_201202_mean.....na.rm.TRUE. = NA
nl_data$NL_VIIRS.M_VCMCFG.MTSALL.MEAN.RGFT_201203_mean.....na.rm.TRUE. = NA

```


```{r PRP}
# Melt Nightlights data by region, district and township

# Drop country and area
nl_data <- nl_data[,-c(1,5)]

# Convert in a dataframe
nl_data <- as.data.frame(nl_data, stringsAsFactors = FALSE)

# Melt the dataset in a more appropriate format for analysis (short)
# It can be that the same township has the same name for different districts, so we need 
# to include that division when melting
nl_data <- reshape2::melt(nl_data, id=c("region", "district", "township"))

# Convert period (in the column named "variable") in date format
nl_data$variable = gsub("[^[:digit:]]", "", nl_data$variable)
nl_data$variable = sub("(.{4})(.*)", "\\1/\\2", nl_data$variable)
nl_data$variable = paste(nl_data$variable, "/01", sep = "")
nl_data$variable = as.Date(nl_data$variable)

# Order by date
nl_data = nl_data[order(nl_data$variable),]

# Assign more identifiable names to the columns
colnames(nl_data) <- c("region", "district", "township", "date", "radiance")

```


```{r EDA, eval=FALSE}
# Explore variability of the readings by township

nl_temp <- nl_data[nl_data$region == parameter_region,]

# dataframe must be ordered by township, year and month to work properly
nl_temp = nl_temp[order(nl_temp$region,
                              nl_temp$district,
                              nl_temp$township,
                              nl_temp$date),]

# calculate the % of change
nl_temp <- nl_temp %>%
    mutate(change = 100 * (radiance - lag(radiance))/lag(radiance))


# plot
nl_plot <- ggplot(nl_temp, aes(date, change)) + geom_point(size=0.8, alpha=0.5)
ggplotly(nl_plot)

rm(nl_temp, nl_plot)

```


```{r PRP}
# Create a dataframe with the NL changes by township and region

# initialize dataframe
nl_changes <- data.frame(region=character(),
                         district=character(),
                         township=character(),
                         first=double(),
                         last=double(),
                         stringsAsFactors=FALSE)

# Changes are normalized by year (like we do with  %  of interest of money):
# the readings change will be divided by the years between the first and last date
length <- as.numeric(difftime(parameter_endDate, parameter_startDate, unit="weeks"))/52.25

for (i in 1:nrow(rg_list)) {
  
  nl_temp = nl_data[which(nl_data$region==rg_list[i,1]), ]
  nl_temp = nl_temp[(nl_temp$date==parameter_startDate | nl_temp$date==parameter_endDate),]
  nl_temp = cast(nl_temp, region+district+township~date, mean, value = "radiance")
  names(nl_temp) = c("region", "district", "township", "first", "last")
  nl_temp$light_score = nl_temp$last/nl_temp$first/length

  # assign levels
  nl_temp = nl_temp %>% 
            mutate(light_level = case_when(
                light_score < -0.3 ~ "Plummeted",
                light_score < -0.05 ~ "Decreased",
                light_score < 0.05 ~ "Similar",
                light_score < 0.3 ~ "Increased",
                light_score >= 0.30 ~ "Rocketed"))
  
  # append the rows to the nl_changes dataframe
  nl_changes = rbind(nl_changes, nl_temp)

}

# remove the variables that we don´t want
rm(i, length, nl_temp)
  
```


```{r LDA}
# Load conflict data

# Each row in the dataset represents an event linked to a conflict
# Each conflict can have several events
# A location can have more than one event
# Each event can last several months
# Deaths are set by event

file <- paste(dir_data, 
                 "conflicts/ged201.xlsx", 
                 sep="", 
                 collapse=NULL)

# Select columns
col_types <- c("text", # id
                  "skip", # relid
                  "numeric", # year
                  "numeric", # active_year
                  "skip", # code_status
                  "numeric", # type_of_violence
                  "skip", # conflict_dset_id
                  "numeric", # conflict_new_id
                  "text", # conflict_name
                  "skip", # dyad_dset_id
                  "numeric", # dyad_new_id
                  "text", # dyad_name
                  "skip", # side_a_dset_id
                  "numeric", # side_a_new_id
                  "text", # side_a
                  "skip", # side_b_dset_id
                  "numeric", # side_b_new_id
                  "text", # side_b
                  "numeric", # number_of_sources
                  "skip", # source_article
                  "skip", # source_office
                  "skip", # source_date
                  "skip", # source_headline
                  "skip", # source_original
                  "numeric", # where_prec
                  "text", # where_coordinates
                  "skip", # where_description
                  "skip", # adm_1
                  "skip", # adm_2
                  "numeric", # latitude
                  "numeric", # longitude
                  "skip", # geom_wkt
                  "skip", # priogrid_gid
                  "text", # country
                  "numeric", # country_id
                  "skip", # region
                  "numeric", # event_clarity
                  "numeric", # date_prec
                  "date", # date_start
                  "date", # date_end
                  "numeric", # deaths_a
                  "numeric", # deaths_b
                  "numeric", # deaths_civilians
                  "numeric", # deaths_unknown
                  "numeric", # best
                  "numeric", # high
                  "numeric", # low
                  "skip", # gwnoa
                  "skip") # gwnob

cn <- read_xlsx(path=file,
               col_names=TRUE,
               col_types=col_types,
               skip=0)

# from the cn data we will create three main dataframes: conflicts, events, and locations

# remove the variables that we don´t need
rm(col_types, file)

```


```{r CLN}
# Create a dataframe of events from Myanmar

# Select events by country
cn_events = as.data.frame(cn[which(cn$country_id==775), ])
cn_events = select(cn_events, -country, -country_id)

# convert to date
cn_events$date_start = as.Date(cn_events$date_start)
cn_events$date_end = as.Date(cn_events$date_end)

# convert to boolean
cn_events$active_year = as.logical(cn_events$active_year)

# convert to factor
cn_events$date_prec = as.factor(cn_events$date_prec)
cn_events$where_prec = as.factor(cn_events$where_prec)
cn_events$event_clarity = as.factor(as.numeric(cn_events$event_clarity))

# Rename type_of_violence to a more meaningful ones
cn_events$type_of_violence[cn_events$type_of_violence == 1] <- "State"
cn_events$type_of_violence[cn_events$type_of_violence == 2] <- "No-state"
cn_events$type_of_violence[cn_events$type_of_violence == 3] <- "1-sided"
cn_events$type_of_violence = as.factor(cn_events$type_of_violence)

# change names
names(cn_events) <- c("id","year","active_year",
                    "type_of_violence","conflict_id","conflict_name","dyad_id","dyad_name",
                    "side_a_id","side_a_name","side_b_id","side_b_name",
                    "number_of_sources",
                    "where_prec","location_name","latitude","longitude",
                    "event_clarity","date_prec","date_start","date_end",
                    "deaths_side_a","deaths_side_b","deaths_civilians",
                    "deaths_unknown","deaths_total","deaths_high","deaths_low")

# convert to integer the deaths numbers
cn_events$deaths_civilians = as.integer(cn_events$deaths_civilians)
cn_events$deaths_side_a = as.integer(cn_events$deaths_side_a)
cn_events$deaths_side_b = as.integer(cn_events$deaths_side_b)
cn_events$deaths_unknown = as.integer(cn_events$deaths_unknown)
cn_events$deaths_high = as.integer(cn_events$deaths_high)
cn_events$deaths_low = as.integer(cn_events$deaths_low)
cn_events$deaths_total = as.integer(cn_events$deaths_total)


```


```{r PRP}
# Add to the events dataframe the length in days of each event

cn_events$length = 0

for (i in 1:nrow(cn_events)) {
  
  cn_events[i, 29] = 
    round(as.numeric(difftime(cn_events[i, 21], 
                              cn_events[i, 20],
                              unit="days"))) + 1

}

# remove the variables that we don´t need
rm(i)

```


```{r EDA, eval=FALSE}
# Check distributions


# histogram
ggplot(cn_events, aes(x=deaths_civilians)) + 
  geom_histogram(binwidth = 1) + 
  scale_x_continuous(breaks = seq(0, max(cn_events$deaths_civilians), 10))

# density + quantiles
cn_plot <- density(cn_events$date_length)
cn_temp <- data.frame(x=cn_plot$x, y=cn_plot$y)

cn_quantiles <- quantile(cn_events$date_length, prob=c(0.1, 0.25, 0.5, 0.75, 0.9))

cn_temp$quant <- factor(findInterval(cn_temp$x, cn_quantiles))
ggplot(cn_temp, aes(x,y)) + 
  geom_line() + 
  geom_ribbon(aes(ymin=0, ymax=y, fill=quant)) + 
  scale_x_continuous(breaks=cn_quantiles) + 
  scale_fill_brewer(guide="none")

# remove variables we are not going to use anymore
rm(cn_quantiles)

```


```{r PRP}
# Classify events by its severity

# Severity of event is measured by # of civilians deaths and # of total deaths
# Low: No deaths.
# Medium: Deaths without civilians.
# High: Deaths of civilians, and maybe non-civilians.

# Score system
# Length of the event: 0 days (0 points), 1-7 days (3 points), 8-30 days (5 points), >30 days (10 points)
# Deaths of civilians: 0 deaths (0 points), 1-10 deaths (20 points), 11-30 (30 points), > 30 deaths (40 points)
# Deaths unknown:  0 deaths (0 points), 1-10 deaths (15 points), 11-30 (20 points), > 30 deaths (30 points)
# Total deaths:  0 deaths (0 points), 1-10 deaths (10 points), 11-30 (15 points), > 30 deaths (20 points)

# Calculate the severity of each event based on the previous system
cn_events$severity_score = 0
cn_events$severity_level = ""

for (i in 1:nrow(cn_events)) {
  
  cn_scoreLength = case_when(
    cn_events[i,29] == 0 ~ 0,
    cn_events[i,29] < 8 ~ 3,
    cn_events[i,29] < 31 ~ 5,
    TRUE ~ 10
  )
  
  cn_scoreCivilians = case_when(
    cn_events[i,24] == 0 ~ 0,
    cn_events[i,24] < 10 ~ 20,
    cn_events[i,24] < 31 ~ 30,
    TRUE ~ 40
  )
  
  cn_scoreUnknown = case_when(
    cn_events[i,25] == 0 ~ 0,
    cn_events[i,25] < 10 ~ 15,
    cn_events[i,25] < 31 ~ 20,
    TRUE ~ 30
  )
  
  cn_scoreTotal = case_when(
    cn_events[i,26] == 0 ~ 0,
    cn_events[i,26] < 10 ~ 10,
    cn_events[i,26] < 31 ~ 15,
    TRUE ~ 20
  )
  
  cn_events[i, 30] = cn_scoreLength + cn_scoreCivilians + cn_scoreUnknown + cn_scoreTotal

  # Assign the level of severity based on the previous score
  cn_events[i,31] = case_when(
    cn_events[i,30] < 20 ~ "Low",
    cn_events[i,30] < 50 ~ "Medium",
    TRUE ~ "High"
  )

}

# Convert severity_level in an ordered factor
cn_events$severity_level = factor(cn_events$severity_level, ordered = TRUE)

# remove the variables that we don´t need
rm(i, cn_scoreLength, cn_scoreCivilians, cn_scoreUnknown, cn_scoreTotal)

```



```{r PRP}
# calculate overall precision of each event

# To calculate it, we will use the following variables:

# where_prec: from 7 (lowest) to 1 (highest). Calculation = 8-X, being X the value of the observation
# number_of_sources: from -1 (lowest) to 44 (highest). Calculation = X+1, being X the value of the observation
# event_clarity: from 1 (highest) to 2 (lowest). Calculation = (3-X)*3, being X the value of the observation
# date_prec: from 1 (lowest) to 5 (highest). Calculation = 6-X, being X the value of the observation

# Calculate the score of the event in terms of precision
cn_events$precision_score = (8-as.numeric(cn_events$where_prec)) + 
  (cn_events$number_of_sources+1) +
  ((3-as.numeric(cn_events$event_clarity))*3) +
  (6-as.numeric(cn_events$date_prec))

# Three levels of precision: low, medium and high
# Low precision score: < 15
# Medium precision score: 16-30
# High precision score: >30

# assign levels
cn_events = cn_events %>% 
  mutate(precision_level = case_when(
    .$precision_score < 15 ~ "Low",
    .$precision_score < 30 ~ "Medium",
    TRUE ~ "High"
    )
  )

# Convert precision_level in an ordered factor
cn_events$precision_level = factor(cn_events$precision_level, ordered = TRUE)

```


```{r EDA}
# check the distribution of the score

plot(density(cn_events$event_score))

```


```{r EDA}
# Exploring conflicts dataset

# Explore deaths
cn_temp = cn_events[c("date_start", "deaths_side_a", "deaths_side_b", "deaths_civilians", "deaths_unknown", "deaths_total","deaths_high", "deaths_low")]
cn_temp = melt(cn_temp, "date_start")

# boxplot
cn_plot <-ggplot(cn_temp, aes(x=variable, y=value)) +
  geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=2, notch=FALSE)

# scatterplot
cn_plot <- ggplot(cn_temp) +
  geom_point(aes(x=date_start, y=value, color=variable), alpha = 0.4) + 
  facet_wrap( ~ variable, ncol=3)

# events by year
cn_temp = cn_events[c("year", "event_prec")]

ggplot(cn_plot, aes(x=year, fill=event_prec)) +
  geom_histogram(colour='white', size=1)

```




```{r PRP}
# Remove points that lay of the polygons (outsiders) in the events dataframe

# create events spatialpolygon with geographical points 
cn_temp <- data.frame(cn_events$longitude, cn_events$latitude)
coordinates(cn_temp) <- ~cn_events.longitude+cn_events.latitude
proj4string(cn_temp) <- proj4string(tw_geo)

# get the numeric index row of outsiders 
cn_events_outsiders = over(cn_temp, tw_geo)
cn_events_outsiders = rownames(cn_events_outsiders[is.na(cn_events_outsiders$GID_0),])
cn_events_outsiders = as.numeric(cn_events_outsiders)

# remove the outsiders from the conflict dataset
cn_events = cn_events %>% 
      filter(!row_number() %in% cn_events_outsiders)

# remove the outsiders from the cn_events dataset (creating it again)
cn_temp <- data.frame(cn_events$longitude, cn_events$latitude)
coordinates(cn_temp) <- ~cn_events.longitude+cn_events.latitude
proj4string(cn_temp) <- proj4string(tw_geo)

# improvement: we may want to include the points that are in the border of the administrative divisions

# remove variables we don´t want anymore
rm(cn_events_outsiders)

```


```{r EDA}

# plot the map and coordinates
{plot(st_geo, col = 'lightgrey', border = 'darkgrey')
points(cn_events$longitude, cn_events$latitude, col="red", cex=1)}


```


```{r PRP}
# Attach the administrative divisions names to the events dataframe

cn_temp$NAME_1 = over(cn_temp, tw_geo)$NAME_1
cn_temp$NAME_2 = over(cn_temp, tw_geo)$NAME_2
cn_temp$NAME_3 = over(cn_temp, tw_geo)$NAME_3

# An alternative for the intersection
#gIntersection(cn_temp, tw_geo)

# convert spatialpointsdataframe to dataframe
cn_temp <-  as.data.frame(cn_temp)
names(cn_temp) <- c("region","district","township","longitude","latitude")

# bind the administrative divisions name to the conflicts dataset
region <- with(cn_events, ifelse(cn_events$longitude == cn_temp$longitude & 
                              cn_events$latitude == cn_temp$latitude, 
                                cn_temp$region, ""))

district <- with(cn_events, ifelse(cn_events$longitude == cn_temp$longitude & 
                              cn_events$latitude == cn_temp$latitude, 
                                cn_temp$district, ""))

township <- with(cn_events, ifelse(cn_events$longitude == cn_temp$longitude & 
                              cn_events$latitude == cn_temp$latitude, 
                                cn_temp$township, ""))

cn_events <- cbind(cn_events, region)
cn_events <- cbind(cn_events, district)
cn_events <- cbind(cn_events, township)

# delete variables that we don't need anymore
rm(region, district, township, cn_temp)

```


```{r EDA}

# plot those that fall in a specific state
filter <- cn_events[ which(cn_events$region=="Kachin"), ]

{plot(st_geo, col = 'lightgrey', border = 'darkgrey')
points(filter$longitude, filter$latitude, col="red", cex=1)}


```


```{r EDA}

# events by type and state
cn_temp = cn_events[c("year", "type_of_violence", "NAME_1")]
cn_temp = cn_temp %>% count(type_of_violence, NAME_1)

ggplot(cn_temp) +
  geom_point(aes(x=n, y=NAME_1, color=type_of_violence), alpha = 0.4)

ggplot(cn_temp, aes(x=n, fill=type_of_violence, color=type_of_violence)) +
  geom_histogram(position="dodge")


```



```{r PRP}
# Create a dataframe with information at conflicts level

# Get unique conflicts by region
cn_conflicts <- unique(cn_events[c("conflict_id", "conflict_name", "region", "type_of_violence")])
cn_conflicts_list <- unique(cn_events[c("conflict_id", "conflict_name", "type_of_violence")])

# Calculating the first and last date by conflict_id
for (i in 1:nrow(cn_conflicts_list)) {

  # Select events by conflict id 
  cn_temp = cn_events %>% filter(conflict_id == cn_conflicts_list[i, 1])
  
  # Calculate the length in weeks
  start = min(cn_temp$date_start)
  end = max(cn_temp$date_start)
  length = round(as.numeric((difftime(end, start, units="weeks"))/52.25), 2)
  
  # assign the start, end and length to each conflict-region row
  for (j in 1:nrow(cn_conflicts)) {

    # assign the data to the conflicts
    if (cn_conflicts[j,1] == cn_conflicts_list[i,1]) {
      cn_conflicts[j,"start"] = start
      cn_conflicts[j,"end"] = end
      cn_conflicts[j,"length"] = length
    }
    
  }

}

# adding the number of events
cn_conflicts$n_events = 0
for (i in 1:nrow(cn_conflicts)){
  
  # calculate the NUMBER OF EVENTS by conflict and region
  cn_conflicts[i, 8] = cn_events %>%
    filter(conflict_id == cn_conflicts[i,1] &
           region == cn_conflicts[i,3]) %>%
    count()
}

# adding severity score and level
cn_conflicts$severity_score = 0
for (i in 1:nrow(cn_conflicts)){
  
  # calculate the NUMBER OF EVENTS by conflict and region
  cn_conflicts[i, 9] = cn_events %>%
    filter(conflict_id == cn_conflicts[i,1]) %>%
    summarise(mean = round(mean(severity_score)))
}

# Assign the level of severity based on the previous score
cn_conflicts$severity_level = ""
for (i in 1:nrow(cn_conflicts)){
  
  cn_conflicts[i,10] = case_when(
    cn_conflicts[i,9] < 20 ~ "Low",
    cn_conflicts[i,9] < 50 ~ "Medium",
    TRUE ~ "High"
  )
  
}

# remove the variables that we don´t need anymore
rm(i, j, start, end, length, cn_temp)


```


```{r PRP}
# Create a dataframe with conflicts information at locations level

# A location can have more than one event
cn_locations = cn_events %>%
  group_by(latitude, longitude) %>%
  summarise(n_events = n(), 
            c_deaths = sum(deaths_civilians),
            u_deaths = sum(deaths_unknown),
            t_deaths = sum(deaths_total))

# add length to the locations
cn_temp = cn_events %>%
  group_by(longitude, latitude) %>%
  summarise(start = min(date_start), 
            end = max(date_end))

# we add one to count one day events
cn_temp$length = round(as.numeric(difftime(cn_temp$end, cn_temp$start, unit="days")))+1

# merge to the locations
cn_locations = merge(cn_locations, cn_temp)

# Attach region to each location
cn_temp = unique(cn_events[with(cn_events, order(latitude, longitude)), c(16, 17, 34)])
cn_locations = merge(cn_locations, cn_temp, all.x=TRUE)

# order columns
cn_locations <- cn_locations[, c("latitude", "longitude", "region",
                                 "n_events", "length", 
                                 "c_deaths", "u_deaths", "t_deaths")]

# remove the variables that we don´t need
rm(cn_temp)


```


```{r PRP}
# Create a dataframe of locations with information detail by conflict and type of violence

cn_locations_detail = cn_events %>%
  group_by(conflict_name, type_of_violence, latitude, longitude, region) %>%
  summarise(n_events = n(), 
            c_deaths = sum(deaths_civilians),
            u_deaths = sum(deaths_unknown),
            t_deaths = sum(deaths_total))

# add length to the locations
cn_temp = cn_events %>%
  group_by(conflict_name, type_of_violence, latitude, longitude, region) %>%
  summarise(start = min(date_start), 
            end = max(date_end))

# we add one to count one day events
cn_temp$length = round(as.numeric(difftime(cn_temp$end, cn_temp$start, unit="days")))+1

# merge to the locations
cn_locations_detail = merge(cn_locations_detail, cn_temp)

# change names
colnames(cn_locations_detail)[1] <- "c_name"
colnames(cn_locations_detail)[2] <- "t_violence"

# order columns
cn_locations_detail <- cn_locations_detail[, c("latitude", "longitude", "region",
                                 "c_name", "t_violence",
                                 "start", "end", "length",
                                 "n_events", "c_deaths", "u_deaths", "t_deaths")]

# remove the variables that we don´t need
rm(cn_temp)

```


```{r PRP}
# Classify locations by its severity

# Severity of event is measured by # of civilians deaths and # of total deaths
# Low: No deaths.
# Medium: Deaths without civilians.
# High: Deaths of civilians, and maybe non-civilians.

# Score system
# Length of the event: 0 days (0 points), 1-7 days (3 points), 8-30 days (5 points), >30 days (10 points)
# Deaths of civilians: 0 deaths (0 points), 1-10 deaths (20 points), 11-30 (30 points), > 30 deaths (40 points)
# Deaths unknown:  0 deaths (0 points), 1-10 deaths (15 points), 11-30 (20 points), > 30 deaths (30 points)
# Total deaths:  0 deaths (0 points), 1-10 deaths (10 points), 11-30 (15 points), > 30 deaths (20 points)

# Calculate the severity of each event based on the previous system
cn_locations$severity_score = 0
cn_locations$severity_level = ""

for (i in 1:nrow(cn_locations)) {
  
  cn_scoreLength = case_when(
    cn_locations[i,5] == 0 ~ 0,
    cn_locations[i,5] < 8 ~ 3,
    cn_locations[i,5] < 31 ~ 5,
    TRUE ~ 10
  )
  
  cn_scoreCivilians = case_when(
    cn_locations[i,6] == 0 ~ 0,
    cn_locations[i,6] < 10 ~ 20,
    cn_locations[i,6] < 31 ~ 30,
    TRUE ~ 40
  )
  
  cn_scoreUnknown = case_when(
    cn_locations[i,7] == 0 ~ 0,
    cn_locations[i,7] < 10 ~ 15,
    cn_locations[i,7] < 31 ~ 20,
    TRUE ~ 30
  )
  
  cn_scoreTotal = case_when(
    cn_locations[i,8] == 0 ~ 0,
    cn_locations[i,8] < 10 ~ 10,
    cn_locations[i,8] < 31 ~ 15,
    TRUE ~ 20
  )
  
  cn_locations[i, 9] = cn_scoreLength + cn_scoreCivilians + cn_scoreUnknown + cn_scoreTotal

  # Assign the level of severity based on the previous score
  cn_locations[i,10] = case_when(
    cn_locations[i,9] < 20 ~ "Low",
    cn_locations[i,9] < 50 ~ "Medium",
    TRUE ~ "High"
  )

}

# Convert severity_level in an ordered factor
cn_locations$severity_level = factor(cn_locations$severity_level, ordered = TRUE)

# Remove the variables that we don´t need anymore
rm(i, cn_scoreLength, cn_scoreCivilians, cn_scoreUnknown, cn_scoreTotal)

```


```{r PRP}
# convert location coordinates to spatial object
coordinates(cn_locations) <- ~longitude+latitude
proj4string(cn_locations) <- proj4string(tw_geo)

```



```{r PRP}
# Classify locations detail by its severity

# Same score system as for locations

# Calculate the severity of each event based on the previous system
cn_locations_detail$severity_score = 0
cn_locations_detail$severity_level = ""

for (i in 1:nrow(cn_locations_detail)) {
  
  cn_scoreLength = case_when(
    cn_locations_detail[i,8] == 0 ~ 0,
    cn_locations_detail[i,8] < 8 ~ 3,
    cn_locations_detail[i,8] < 31 ~ 5,
    TRUE ~ 10
  )
  
  cn_scoreCivilians = case_when(
    cn_locations_detail[i,10] == 0 ~ 0,
    cn_locations_detail[i,10] < 10 ~ 20,
    cn_locations_detail[i,10] < 31 ~ 30,
    TRUE ~ 40
  )
  
  cn_scoreUnknown = case_when(
    cn_locations_detail[i,11] == 0 ~ 0,
    cn_locations_detail[i,11] < 10 ~ 15,
    cn_locations_detail[i,11] < 31 ~ 20,
    TRUE ~ 30
  )
  
  cn_scoreTotal = case_when(
    cn_locations_detail[i,12] == 0 ~ 0,
    cn_locations_detail[i,12] < 10 ~ 10,
    cn_locations_detail[i,12] < 31 ~ 15,
    TRUE ~ 20
  )
  
  cn_locations_detail[i, 13] = cn_scoreLength + cn_scoreCivilians + cn_scoreUnknown + cn_scoreTotal

  # Assign the level of severity based on the previous score
  cn_locations_detail[i,14] = case_when(
    cn_locations_detail[i,13] < 20 ~ "Low",
    cn_locations_detail[i,13] < 50 ~ "Medium",
    TRUE ~ "High"
  )

}

# Convert severity_level in an ordered factor
cn_locations_detail$severity_level = factor(cn_locations_detail$severity_level, ordered = TRUE)

# Remove the variables that we don´t need anymore
rm(i, cn_scoreLength, cn_scoreCivilians, cn_scoreUnknown, cn_scoreTotal)

```



```{r PRP}
# convert location detail coordinates to spatial object
coordinates(cn_locations_detail) <- ~longitude+latitude
proj4string(cn_locations_detail) <- proj4string(tw_geo)

```


```{r MOD}
# Create location clusters based on distance and impact

# https://blog.dominodatalab.com/geographic-visualization-with-rs-ggmaps/
# https://gis.stackexchange.com/questions/17638/clustering-spatial-data-in-r
# https://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/
# https://en.proft.me/2017/02/1/model-based-clustering-r/
  
# Prepare de dataset for clustering
loc = cn_events[cn_events$region==parameter_region,c(16,17,31)]
loc$severity_level= as.character(loc$severity_level)
loc$severity_level[loc$severity_level == "High"] <- "3"
loc$severity_level[loc$severity_level == "Medium"] <- "2"
loc$severity_level[loc$severity_level == "Low"] <- "1"
loc$severity_level = as.numeric(loc$severity_level)

l = loc[,-c(3)]

# Perform model-based clustering (only over numerical variables) 
md_mbc = Mclust(l)

# Check results
md_mbc$modelName # optimal selected model
md_mbc$G # # optimal number of cluster
head(md_mbc$z) # probability for an observation to be in a given cluster
summary(md_mbc, parameters = TRUE) # probabilities, means, variances

# Plot the model based on clustering results
plot(md_mbc, what=c("classification"))
plot(md_mbc, "density")

# cluster observations distribution
table(loc$severity_level, md_mbc$classification) # amount of the data within each cluster
data.frame(l,cluster=md_mbc$classification) # clusters assigned to each location

# We will leave for future plot this clusters over the map as now is difficult and computing costly




```


```{r EDA}
# Visualizing the exploring viz from previous dataframe

cn_explore = cn_events_explore[cn_events_explore$NAME_1==parameter_region,]

cn_explore = cn_explore %>%
 filter(date_start >= parameter_date)

# Cleveland Plot
ggplot(cn_explore) +
  geom_segment( aes(x=deaths_total, xend=deaths_total, y=date_start, yend=date_end), color="grey") +
  geom_point( aes(x=deaths_total, y=date_start), color=rgb(0.2,0.7,0.1,0.5), size=1 ) +
  geom_point( aes(x=deaths_total, y=date_end), color=rgb(0.7,0.2,0.1,0.5), size=3) +
  coord_flip()+
  theme(
    legend.position = "none",
    plot.background = element_blank(),
    panel.background = element_rect(fill = "#FFFFFF"),
    panel.border = element_blank(),
    panel.grid = element_line(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_line(size = 0.5, 
                                      linetype = 'dotted',
                                      colour = "black"), 
    panel.grid.major.y = element_line(size = 0.5, 
                                      linetype = 'dotted',
                                      colour = "black"), 
    panel.grid.minor.x = element_blank(),
    panel.grid.minor.y = element_blank()
  ) +
  xlab("") +
  ylab("")


```


```{r PRP}
# Create a dataset with monthly deaths by region from the conflict dataset

# Split the months from the conflicts dataset (in the new columns start_dates and end_dates)
cn_events_monthly <- cn_events %>%
  mutate(test = map2(date_start, date_end, expand_dates)) %>%
  unnest(cols = c(test))

# Add a column with the month
cn_events_monthly$month = month(cn_events_monthly$start_dates)


```


```{r EDA}
# Plotting monthly conflicts by state

# Select a state
cn_explore = cn_events_monthly[ which(cn_events_monthly$NAME_1==parameter_region),]

# Line graph
ggplot(cn_explore, aes(start_dates, deaths_total)) + geom_line() + scale_x_date(date_labels = "%b-%Y") +
geom_vline(xintercept = as.Date("01-01-2018",  format = "%d-%m-%Y"), linetype = 'dotted', color = 'blue')

# Select the data we need to  draw the map
cn_explore = cn_explore[c("id","longitude","latitude","deaths_total","severity")]
# transform the data to spatial coordinates
coordinates(cn_explore) <- ~longitude+latitude

st_layer = st_geo[st_geo@data$NAME_1==parameter_region,]
tw_layer = tw_geo[tw_geo@data$NAME_1==parameter_region,]

# Map of the selected state with the total_deaths as bubbles
tm_shape(st_layer) +
    tm_borders(col="black", lwd=1) +
tm_shape(tw_layer) +
    tm_borders(col="grey", lwd=1, lty="dashed", alpha=0.6) +
tm_shape(cn_explore) +
    tm_bubbles(size="deaths_total", col="severity") + 
tm_legend(show=TRUE) +
tm_layout(frame=FALSE)


```


```{r LDA}
# Load population data from the census

file <- paste(dir_data,
                 "population/BaselineData_Census_Dataset_SR_District_Township_MIMU_16Jun2016_ENG.xlsx", 
                 sep="", 
                 collapse=NULL)

# Select columns
col_types <- c("text", # State/Region Pcode
                  "text", # State/Region name
                  "text", # District Pcode
                  "text", # District Name
                  "text", # Township Pcode
                  "text", # Township Name
                  "numeric", # Population total
                  "skip", # Population male
                  "skip", # Population female
                  "skip", # Population sex ratio
                  "skip", # Population total urban
                  "skip", # Population male urban
                  "skip", # Population female urban
                  "skip", # Population sex ratio urban
                  "skip", # Population total rural
                  "skip", # Population male rural
                  "skip", # Population female rural
                  "skip", # Population sex ratio rural
                  "skip") # Urban Population %

pp <- read_xlsx(path=file,
                 sheet = "Table A-3",
                 col_names=TRUE,
                 col_types=col_types,
                 skip=4)

# remove the variables that we don´t need
rm(file, col_types)


```


```{r CLN}
# Population dataset cleaning

# change columns names
pp_data = pp %>% select(2,4,6,7)
names(pp_data) = c("region", "district", "township", "population")

# remove NAs (population aggregated at regional level)
pp_data = pp_data[complete.cases(pp_data),]

sum(pp_data$population, na.rm=TRUE)
#50,279,900

```


```{r LDA, eval=FALSE}
# Load the file with the mapping of township names from GADM and from census data

# Select columns
col_types <- c("numeric", # GADM code
                  "text", # State/Region name
                  "text", # Township Name
                  "numeric", # Census code
                  "text", # State/Region name
                  "text", # township name
                  "text", # Strategy
                  "numeric", # Target
                  "skip") # Notes

file = paste(dir_data,
                "population/2014 Census Administrative Differences.xlsx",
                sep="",
                collapse=NULL)

tw_mapping <- read_xlsx(path=file,
                 sheet = "Mapping",
                 col_names=TRUE,
                 col_types=col_types,
                 skip=2)

# create a temp dataframe with the ids to link the population
pp_temp = pp_data
pp_temp$id = rownames(pp_temp)

# remove the variables that we don´t need
rm(file, col_types)


```



```{r PRP}
# Create a dataframe to store the predictions

# use nl data structure
pp_predictions = nl_data

# attach the area from the nightlights dataframe
pp_predictions = merge(x=pp_predictions, 
                 y=nl[,c("division_.yin.","district_.kayaing.","village.township","area_sq_km")],
                 by=c("region","district","township"),
                 by.y=c("division_.yin.","district_.kayaing.","village.township"),
                 all.x=TRUE)

colnames(pp_predictions)[6] <- "area"

#split the date
pp_predictions$month = month(pp_predictions$date)
pp_predictions$year = year(pp_predictions$date)
pp_predictions$date = NULL

#order by date
pp_predictions = pp_predictions[order(pp_predictions$year, pp_predictions$month),]

# Order the events by month and year
cn_temp = cn_events
cn_temp$month = month(cn_temp$date_start)
cn_temp = cn_temp[order(cn_temp$year, cn_temp$month),]

# add the new columns as features 
pp_predictions$n_events = 0
pp_predictions$c_deaths = 0
pp_predictions$t_deaths = 0
pp_predictions$pop = 0

for (i in 1:nrow(pp_predictions)){
  
  # calculate the NUMBER OF EVENTS by township and date
  # (update with district to catch duplicated townships)
  pp_predictions[i, 8] = cn_temp %>%
    filter(region == pp_predictions[i,1] &
           township == pp_predictions[i,3] &
           month == pp_predictions[i,6] &
           year == pp_predictions[i,7]) %>%
    count()

  # calculate the number of CIVILIAN DEATHS by township and date
  # (update with district to catch duplicated townships)
  pp_predictions[i, 9] = cn_temp %>%
    filter(region == pp_predictions[i,1] &
           township == pp_predictions[i,3] &
           month == pp_predictions[i,6] &
           year == pp_predictions[i,7]) %>%
    summarise(d = sum(deaths_civilians))


  # calculate the number of TOTAL DEATHS by township and date
  # (update with district to catch duplicated townships)
  pp_predictions[i, 10] = cn_temp %>%
    filter(region == pp_predictions[i,1] &
           township == pp_predictions[i,3] &
           month == pp_predictions[i,6] &
           year == pp_predictions[i,7]) %>%
    summarise(d = sum(deaths_total))

}

sum(pp_predictions$n_events, na.rm=TRUE)
#734 from April 2012, 756 from January-2012 (2,757 total)

sum(pp_predictions$t_deaths, na.rm=TRUE)
#5,239 from April-2012, 5,568 from January-2012

# re-organize the columns
col_order <- c("region", "district", "township", "area",
               "month", "year", 
               "n_events", "c_deaths", "t_deaths",
               "radiance", "pop")
pp_predictions <- pp_predictions[,col_order]

# remove the variables that we don´t need
rm(cn_temp, i, col_order)

```


```{r CLN}
# Assign the same names from GADM to the population data

# The census was collected between 30 March and 10 April 2014
census_date = as.Date("2014-04-01")

# The census data has been collected using different geographical divisions than the ones in GADM,
# so we need to adjust them to fit census to GADM distribution.
# There will be 4 different strategies to match the data from the census with GADM:

# 1) Direct transfer, when the divisions match in both census and GADM
# 2) Merge with GADM, when one division in the census is completely inside another region of GADM
# 3) Split to GADM, when a division in the census doesn't exist in GADM
# 4) Split from census, when a division in GADM doesn't exist in the census

```


```{r CLN}
# Assign the same names from GADM to the population data
# 1) Direct transfer, when the townships match in both census and GADM

# Select the townships we are going to transfer
tw_mapping_selected = tw_mapping[tw_mapping$Strategy == "Direct Transfer",]

# add population to mapping from the pop_table
tw_mapping_selected = merge(tw_mapping_selected, pp_temp, by.x="census", by.y="id")

for (i in 1:nrow(pp_predictions)){

    # control that is the period from the census
    if ((as.numeric(pp_predictions[i,5]) == as.numeric(month(census_date))) &&
        (as.numeric(pp_predictions[i,6]) == as.numeric(year(census_date)))) {
      
        pp_predictions[i,11] = 
            tw_mapping_selected %>%
            filter(
              NAME_1 == pp_predictions[i,1] &
              NAME_3 == pp_predictions[i,3]) %>%
            summarise(p = sum(population))
    } 
}

sum(pp_predictions$pop, na.rm=TRUE)
# 41,592,810

```


```{r CLN}
# Assign the same names from GADM to the population data

# 2) Merge with GADM, when one division in the census is completely inside another region of GADM

# Select the townships we are going to transfer, those that start with "merge with"
tw_mapping_selected = subset(tw_mapping, grepl("Merge with", Strategy))

# add region and township to mapping from the tw_list
tw_mapping_selected = merge(tw_mapping_selected, tw_list, by.x="Target", by.y="id")

# add population to mapping from the pp_temp
tw_mapping_selected = merge(tw_mapping_selected, pp_temp, by.x="census", by.y="id")

# Top up the population
for (i in 1:nrow(tw_mapping_selected)){
    
    # find the record in the pp_predictions
    for (j in 1:nrow(pp_predictions)){
      
      if (
          (as.numeric(pp_predictions[j,5]) == as.numeric(month(census_date))) &&
          (as.numeric(pp_predictions[j,6]) == as.numeric(year(census_date))) &&
          (pp_predictions[j,1] == tw_mapping_selected[i,9]) &&
          (pp_predictions[j,3] == tw_mapping_selected[i,11])){
  
            pp_predictions[j,11] = pp_predictions[j,11] + tw_mapping_selected[i,15]
            break
      }
    }
}

sum(pp_predictions$pop, na.rm=TRUE)
#49,145,022

# remove variables that we don´t need
rm(i, j)

```


```{r CLN}
# Assign the same names from GADM to the population data

# 3) Split to GADM, when a division in the census doesn't exist in GADM
# 4) Split from census, when a division in GADM doesn't exist in the census

# There are only 5 cases here, so we do it manually
tw_mapping_selected <- subset(tw_mapping, grepl("Split to", Strategy))

# add population to mapping from the pop_table
tw_mapping_selected = merge(tw_mapping_selected, pp_temp, by.x="census", by.y="id")

# case 1: Myinmu to ADD Myinmu (50%) and ADD to Ngazun (50%)
pp_predictions[which(pp_predictions$region=="Sagaing" &
                          pp_predictions$township == "Myinmu" &
                          pp_predictions$month == 4 &
                          pp_predictions$year == 2014), 11] <- round(tw_mapping_selected[1,12]*0.5)

pp_predictions[which(pp_predictions$region=="Sagaing" &
                          pp_predictions$township == "Ngazun" &
                          pp_predictions$month == 4 &
                          pp_predictions$year == 2014), 11] <- round(tw_mapping_selected[1,12]*0.5)

# case 2: Kanbalu ADD to Kanbalu (80%) and ADD to Tantabin (20%)
pp_predictions[which(pp_predictions$region=="Sagaing" &
                          pp_predictions$township == "Kanbalu" &
                          pp_predictions$month == 4 &
                          pp_predictions$year == 2014), 11] <- round(tw_mapping_selected[2,12]*0.8)

pp_predictions[which(pp_predictions$region=="Sagaing" &
                          pp_predictions$township == "Tantabin" &
                          pp_predictions$month == 4 &
                          pp_predictions$year == 2014), 11] <- round(tw_mapping_selected[2,12]*0.2)

# case 3: Kangyidaunt TOP UP to Bassein West (40%) and TOP UP to Ngaputaw (60%)
# store the current population for Bassein West
pp_base <- pp_predictions[which(pp_predictions$region=="Ayeyarwady" &
                          pp_predictions$township == "Bassein West" &
                          pp_predictions$month == 4 &
                          pp_predictions$year == 2014), 11]

pp_predictions[which(pp_predictions$region=="Ayeyarwady" &
                          pp_predictions$township == "Bassein West" &
                          pp_predictions$month == 4 &
                          pp_predictions$year == 2014), 11] <- pp_base + round(tw_mapping_selected[3,12]*0.4)

# store the current population for Ngaputaw
pp_base <- pp_predictions[which(pp_predictions$region=="Ayeyarwady" &
                          pp_predictions$township == "Ngaputaw" &
                          pp_predictions$month == 4 &
                          pp_predictions$year == 2014), 11]

pp_predictions[which(pp_predictions$region=="Ayeyarwady" &
                          pp_predictions$township == "Ngaputaw" &
                          pp_predictions$month == 4 &
                          pp_predictions$year == 2014), 11] <- pp_base + round(tw_mapping_selected[3,12]*0.6)

# case 4: Nyaungdon ADD to Yandoon (60%) and ADD to Irrawaddy (40%)
pp_predictions[which(pp_predictions$region=="Ayeyarwady" &
                          pp_predictions$township == "Yandoon" &
                          pp_predictions$month == 4 &
                          pp_predictions$year == 2014), 11] <- round(tw_mapping_selected[4,12]*0.6)

pp_predictions[which(pp_predictions$region=="Ayeyarwady" &
                          pp_predictions$township == "Irrawaddy" &
                          pp_predictions$month == 4 &
                          pp_predictions$year == 2014), 11] <- round(tw_mapping_selected[4,12]*0.4)

# case 5: Hinthada ADD to Henzada (50%) and to Danubyu (50%)
pp_predictions[which(pp_predictions$region=="Ayeyarwady" &
                          pp_predictions$township == "Henzada" &
                          pp_predictions$month == 4 &
                          pp_predictions$year == 2014), 11] <- round(tw_mapping_selected[5,12]*0.5)

pp_predictions[which(pp_predictions$region=="Ayeyarwady" &
                          pp_predictions$township == "Danubyu" &
                          pp_predictions$month == 4 &
                          pp_predictions$year == 2014), 11] <- round(tw_mapping_selected[5,12]*0.5)

sum(pp_predictions$pop, na.rm=TRUE)
#50,279,900

# remove variables that we don´t need anymore
rm(census_date, pp_temp, tw_mapping, tw_mapping_selected, pp_base)

```


```{r MOD}
# Create a model that predicts people movements from radiance and conflict data

# For this problem we wan´t to infer population based on radiance, events and deaths
# This prediction will be made by region, month and year
# Once we have population, we will automatically calculate the % of change

# The selected features are n_events_c_deaths, u_deaths, t_deaths, and radiance
# The variable to predict is population

# add month to the model

# https://srnghn.medium.com/machine-learning-trying-to-predict-a-numerical-value-8aafb9ad4d36

```


```{r MOD}
# Linear Regression

#http://r-statistics.co/Linear-Regression.html
#http://r-statistics.co/adv-regression-models.html

set.seed(100)

# initialize models evaluation results dataframe
md_lm_evaluation <- data.frame(region=character(),
                            MSE=double(),
                            RMSE=double(),
                            AIC=double(),
                            corrAccuracy=double(),
                            min_max_accuracy=double(),
                            mape=double(),
                            stringsAsFactors=FALSE)

# initialize list to store the models
col_names <- c("region", "model")
md_lm_list <- vector(mode = "list", length = 2)
names(md_lm_list) <- col_names

# create models by region
for (i in 1:nrow(rg_list)) {

  # Select observations for each township where we have population to build the model
  md_pp_temp <- pp_predictions[pp_predictions$region == rg_list[i,1] &
                      pp_predictions$pop > 0,]

  # As some regions haven´t enough observations, we skip them in the model
  if (nrow(md_pp_temp) > 2) {
    
    # Split in training and test data
    md_pp_index <- sample(1:nrow(md_pp_temp), 0.8*nrow(md_pp_temp))
    md_pp_train <- md_pp_temp[md_pp_index, ] # training data
    md_pp_test  <- md_pp_temp[-md_pp_index, ] # test data
    
    # Build the model using training data
    md_lm <- lm(pop ~ radiance+area+n_events+c_deaths+t_deaths, data=md_pp_train)
    
    # Store the results of the model performance
    md_lm_evaluation[i,1] = rg_list[i,1] # region
  
    md_lm_evaluation[i,2] = c(crossprod(md_lm$residuals)) / length(md_lm$residuals) # MSE
    md_lm_evaluation[i,3] = sqrt(c(crossprod(md_lm$residuals)) / length(md_lm$residuals)) # RMSE
    md_lm_evaluation[i,4] = AIC(md_lm) # AIC
    
    # Generate the predictions on the test data
    md_predictions <- predict(md_lm, md_pp_test)
    
    # Attach previous predictions to the real values to measure accuracy
    md_predictions <- data.frame(cbind(actual=md_pp_test$pop, predicted=md_predictions))
    
    # Store the results of the predictions accuracy
    md_lm_evaluation[i,5] <- cor(md_predictions$actual, md_predictions$predicted) # corrAccuracy
    md_lm_evaluation[i,6] <- mean(apply(md_predictions, 1, min) / apply(md_predictions, 1, max)) # min_max_accuracy 
    md_lm_evaluation[i,7] <- mean(abs((md_predictions$predicted - md_predictions$actual))/md_predictions$actual) # mape
    
    # Append the model to the list
    md_lm_list$region[[i]] <- rg_list[i,1]
    md_lm_list$model[[i]] <- md_lm
    
  } else {
    
    md_lm_evaluation[i,1] <- rg_list[i,1]

    md_lm_list$region[[i]] <- rg_list[i,1]
    md_lm_list$model[[i]] <- NULL

  }

}

# remove the variables that we don´t need
rm(md_pp_temp, md_pp_index, md_pp_train, md_pp_test, md_lm, md_predictions, i, col_names)

```


```{r MOD}
# Use models to predict data on observations without population data available

# initialize dataframe where we will store ONLY  the predictions of the model
md_lm_predictions <- data.frame(region=character(),
                            district=character(),
                            township=character(),
                            month=integer(),
                            year=integer(),
                            population=integer(),
                            stringsAsFactors=FALSE)


# Predict data for each township applying the regional model
for (i in 1:nrow(rg_list)) {
  
  # We don´t predict those regions without a model
  if (!is.null(md_lm_list$model[[i]])) {
    
    # Subset the real observations where we don´t have population by region 
    md_pp_temp <- pp_predictions[pp_predictions$region == rg_list[i,1] &
                          pp_predictions$pop == 0,]
    md_pp_temp$pop <- NULL
    
    # For each township/period in the region, apply the regional model
    md_predictions <- predict(md_lm_list$model[[i]], md_pp_temp)
    
    # attach the predictions column to the temp dataframe
    md_pp_temp <- data.frame(cbind(md_pp_temp, md_predictions))
  
    # add the current predicted rows to the predictions dataframe
    md_lm_predictions = data.frame(rbind(md_lm_predictions, md_pp_temp))
    
  }

}

# merge the predictions dataframe to the real observations dataframe
pp_predictions = merge(pp_predictions, md_lm_predictions, all.x=TRUE)
pp_predictions$population = if_else(pp_predictions$pop==0,
                             pp_predictions$md_predictions, 
                             pp_predictions$pop)

# remove the columns that we don´t want
pp_predictions = pp_predictions[,-c(11,12)]

# remove the variables that we don´t need
rm(md_pp_temp, md_predictions, i)

```


```{r PRP}
# Create the dataframe with population changes

# population changes will be shown as month by month change
# we will use the results of the model predictions
# values calculated as a year average by district
pp_changes <- pp_predictions

# Drop the columns that we don´t need
pp_changes = pp_changes[,-c(4,7,8,9,10)]

# dataframe must be ordered by township, year and month to work properly
pp_changes = pp_changes[order(pp_changes$region,
                              pp_changes$district,
                              pp_changes$township,
                              pp_changes$year,
                              pp_changes$month),]

# calculate the % of change
pp_changes <- pp_changes %>%
    mutate(lights_score = 100 * (population - lag(population))/lag(population))

# assign levels of change
pp_changes = pp_changes %>% 
            mutate(light_level = case_when(
                lights_score < -0.3 ~ "Plummeted",
                lights_score < -0.05 ~ "Decreased",
                lights_score < 0.05 ~ "Similar",
                lights_score < 0.3 ~ "Increased",
                lights_score >= 0.30 ~ "Rocketed"))
  
```


```{r EDA}
# Prediction Plot

changesPalette = c("#00dfff","#D6EFF6","#F2F3EC","#fdfce1","#fbf79b")
names(changesPalette) = c("Plummeted", "Decreased", "Similar", "Increased", "Rocketed")
changesScale <- scale_colour_manual(name = "light_level",values = changesPalette)

pp_temp = pp_changes[pp_changes$region == parameter_region,]

p <- ggplot(pp_temp, aes(month, township, fill=light_level)) +
    geom_tile(color="grey", size=0.1) + 
    scale_fill_manual(values=changesPalette, na.value="white") +
    facet_grid(.~year)

p <- p + theme(
    panel.spacing.x=unit(0.01, "lines"), 
    panel.spacing.y=unit(0.01, "lines"),
    panel.background = element_rect(fill = "#F5F5F5", color = "#F5F5F5"),
    plot.background = element_rect(fill = "#F5F5F5", color = "#F5F5F5"),
    plot.title=element_blank(),
    strip.background = element_blank(),
    legend.position = "none",
    axis.title.y=element_text(size=7, 
                              colour="dark grey",
                              margin = margin(t = 0, r = 15, b = 0, l = 0)),
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank(),
    axis.title.x=element_blank(),
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(),
    axis.text=element_blank(),
    strip.text.x = element_text(size=6, colour="dark grey", angle=90)
) +
    xlab("") + 
    ylab("townships") +
    removeGrid()

ggplotly(p) 


```


```{r EDA}
# Prediction plot using plotly without conversion to speed up the process

#https://stackoverflow.com/questions/60763006/improve-performance-of-ggplotly-when-plotting-time-series-heatmap
# https://community.plotly.com/t/poor-javascript-heatmap-performance/3683/6
  
```


```{r PRP}
# Save files and variables

# Export files for the web app

# Save geographical shapes simplified
file = "E:/Data Lights/Shiny/rg_geo.rds"
saveRDS(rg_geo, file=file)

file = "E:/Data Lights/Shiny/tw_geo.rds"
saveRDS(tw_geo, file=file)

# Save dataframes
file = "E:/Data Lights/Shiny/nl_data.rda"
save(nl_data, file=file)

file = "E:/Data Lights/Shiny/nl_changes.rda"
save(nl_changes, file=file)

file = "E:/Data Lights/Shiny/cn_events.rda"
save(cn_events, file=file)

file = "E:/Data Lights/Shiny/cn_conflicts.rda"
save(cn_conflicts, file=file)

file = "E:/Data Lights/Shiny/cn_conflicts_list.rda"
save(cn_conflicts_list, file=file)

file = "E:/Data Lights/Shiny/cn_locations.rda"
save(cn_locations, file=file)

file = "E:/Data Lights/Shiny/cn_locations_detail.rda"
save(cn_locations_detail, file=file)

file = "E:/Data Lights/Shiny/pp_changes.rda"
save(pp_changes, file=file)

# Save global environment
file = paste0(dirname(getwd()), "/myEnvironment.RData", sep="")
save.image(file=file)

# remove variables that we don´t need
rm(file)

```

  
```{r EDA, eval=FALSE}
# App performance

# https://mastering-shiny.org/performance.html
# shinyloadtest::record_session("http://127.0.0.1:3729", output_file = "Logs/recording.log")

# run in PowerShell this command (where shinycannon is placed, not need to "install" it)
java -jar shinycannon-1.0.0-9b22a92.jar "E:/Data Lights/recording.log" http://127.0.0.1:3729 --workers 3 --loaded-duration-minutes 1 --output-dir run1

# load the results
df <- load_runs("E:/Data Lights/run1")

# show the data
shinyloadtest_report(df, "report.html")

```
